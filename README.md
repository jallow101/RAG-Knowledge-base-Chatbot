# ğŸ“š RAG Knowledge Base Chatbot

A **Retrieval-Augmented Generation (RAG)** chatbot built with **Streamlit**, **LangChain**, and **OpenAI**. This application allows users to upload PDF documents to a secure knowledge base and ask questions, receiving answers based solely on the content of those documents.

## ğŸš€ Features

- **ğŸ“„ PDF Ingestion**: Upload and process PDF documents automatically.
- **ğŸ” Admin Panel**: Secure area for managing the knowledge base (uploading files).
- **ğŸ§  RAG Architecture**: Uses OpenAI's `gpt-4o-mini` and `text-embedding-3-small` for high-quality retrieval and generation.
- **âš¡ FAISS Vector Store**: Fast similarity search for retrieving relevant document chunks.
- **ğŸ’¬ Interactive Chat**: Simple and responsive chat interface using Streamlit.

## ğŸ› ï¸ Tech Stack

- **Python 3.8+**
- **Streamlit** (UI)
- **LangChain** (RAG Framework)
- **OpenAI API** (LLM & Embeddings)
- **FAISS** (Vector Database)
- **PyPDF2** (PDF Processing)

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ app/
â”‚   â””â”€â”€ streamlit_app.py    # Main application entry point
â”œâ”€â”€ data/                   # Data storage
â”‚   â”œâ”€â”€ raw/                # Uploaded PDF files
â”‚   â”œâ”€â”€ processed/          # Extracted text files
â”‚   â””â”€â”€ vectorstore/        # FAISS vector index
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py           # Configuration settings
â”‚   â”œâ”€â”€ ingest.py           # PDF processing logic
â”‚   â”œâ”€â”€ chunker.py          # Text chunking logic
â”‚   â”œâ”€â”€ vector_store.py     # FAISS vector store operations
â”‚   â””â”€â”€ rag_chain.py        # RAG pipeline definition
â”œâ”€â”€ .env.example            # Example environment variables
â””â”€â”€ README.md               # Project documentation
```

## âš™ï¸ Installation & Setup

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd RAG-Knowledge-base-Chatbot
   ```

2. **Create a virtual environment (optional but recommended)**
   ```bash
   python -m venv venv
   # Windows
   venv\Scripts\activate
   # Mac/Linux
   source venv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   pip install streamlit langchain-openai langchain-community faiss-cpu python-dotenv PyPDF2
   ```

4. **Configure Environment Variables**
   Create a `.env` file in the root directory:
   ```bash
   # Copy the example file (or create new)
   cp .env.example .env
   ```
   
   Open `.env` and add your OpenAI API key:
   ```env
   OPENAI_API_KEY=sk-proj-...
   ADMIN_PASSWORD=adminAi  # Change this to secure your admin panel
   ```

## â–¶ï¸ Usage

1. **Run the Application**
   ```bash
   streamlit run app/streamlit_app.py
   ```

2. **Admin Mode (Upload Data)**
   - Open the app in your browser (usually `http://localhost:8501`).
   - Go to the **Sidebar**.
   - Enter the Admin Password (default: `adminAi`).
   - Upload PDF files. They will be automatically chunked and indexed.

3. **User Mode (Chat)**
   - Type your question in the main chat input.
   - The bot will answer based *only* on the uploaded PDFs.

## ğŸ§© Configuration

You can tweak settings in `src/config.py`:
- `CHUNK_SIZE`: Size of text chunks (default: 1200).
- `CHUNK_OVERLAP`: Overlap between chunks (default: 200).
- `LLM_MODEL`: OpenAI model to use (default: `gpt-4o-mini`).
